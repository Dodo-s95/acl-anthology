<?xml version='1.0' encoding='UTF-8'?>
<collection id="2023.jeptalnrecital">
  <volume id="coria" ingest-date="2023-06-25">
    <meta>
      <booktitle>Actes de CORIA-TALN 2023. Actes de la 18e Conférence en Recherche d'Information et Applications (CORIA)</booktitle>
      <editor><first>Haïfa</first><last>Zargayouna</last></editor>
      <publisher>ATALA</publisher>
      <address>Paris, France</address>
      <month>6</month>
      <year>2023</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="50d313b5">2023.jeptalnrecital-coria.0</url>
      <bibkey>jep-taln-recital-2023-actes</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Impact de l’apprentissage multi-labels actif appliqué aux transformers</title>
      <author><first>Maxime</first><last>Arens</last></author>
      <author><first>Charles</first><last>Teissèdre</last></author>
      <author><first>Lucile</first><last>Callebert</last></author>
      <author><first>Jose G</first><last>Moreno</last></author>
      <author><first>Mohand</first><last>Boughanem</last></author>
      <pages>2–17</pages>
      <abstract>L’Apprentissage Actif (AA) est largement utilisé en apprentissage automatique afin de réduire l’effort d’annotation. Bien que la plupart des travaux d’AA soient antérieurs aux transformers, le succès récent de ces architectures a conduit la communauté à revisiter l’AA dans le contexte des modèles de langues pré-entraînés.De plus, le mécanisme de fine-tuning, où seules quelques données annotées sont utilisées pour entraîner le modèle sur une nouvelle tâche, est parfaitement en accord avec l’objectif de l’AA. Nous proposons d’étudier l’impact de l’AA dans le contexte des transformers pour la tâche de classification multi-labels. Or la plupart des stratégies AA, lorsqu’elles sont appliquées à ces modèles, conduisent à des temps de calcul excessifs, ce qui empêche leur utilisation au cours d’une interaction homme-machine en temps réel. Afin de pallier ce problème, nous utilisons des stratégies d’AA basées sur l’incertitude. L’article compare six stratégies d’AA basées sur l’incertitude dans le contexte des transformers et montre que si deux stratégies améliorent invariablement les performances, les autres ne surpassent pas l’échantillonnage aléatoire. L’étude montre également que les stratégies performantes ont tendance à sélectionner des ensembles d’instances plus diversifiées pour l’annotation.</abstract>
      <url hash="58e2263e">2023.jeptalnrecital-coria.1</url>
      <language>fra</language>
      <bibkey>arens-etal-2023-impact</bibkey>
    </paper>
    <paper id="2">
      <title>Quelles évolutions sur cette loi ? Entre abstraction et hallucination dans le domaine du résumé de textes juridiques</title>
      <author><first>Nihed</first><last>Bendahman</last></author>
      <author><first>Karen</first><last>Pinel-Sauvagnat</last></author>
      <author><first>Gilles</first><last>Hubert</last></author>
      <author><first>Mokhtar Boumedyen</first><last>Billami</last></author>
      <pages>18–36</pages>
      <abstract>Résumer automatiquement des textes juridiques permettrait aux chargés de veille d’éviter une surcharge informationnelle et de gagner du temps sur une activité particulièrement chronophage. Dans cet article, nous présentons un corpus de textes juridiques en français associés à des résumés de référence produits par des experts, et cherchons à établir quels modèles génératifs de résumé sont les plus intéressants sur ces documents possédant de fortes spécificités métier. Nous étudions quatre modèles de l’état de l’art, que nous commençons à évaluer avec des métriques traditionnelles. Afin de comprendre en détail la capacité des modèles à transcrire les spécificités métiers, nous effectuons une analyse plus fine sur les entités d’intérêt. Nous évaluons notamment la couverture des résumés en termes d’entités, mais aussi l’apparition d’informations non présentes dans les documents d’origine, dites hallucinations. Les premiers résultats montrent que le contrôle des hallucinations est crucial dans les domaines de spécialité, particulièrement le juridique.</abstract>
      <url hash="c0917a39">2023.jeptalnrecital-coria.2</url>
      <language>fra</language>
      <bibkey>bendahman-etal-2023-quelles</bibkey>
    </paper>
    <paper id="3">
      <title>Augmentation de jeux de données <fixed-case>RI</fixed-case> pour la recherche conversationnelle à initiative mixte</title>
      <author><first>Pierre</first><last>Erbacher</last></author>
      <author><first>Philippe</first><last>Preux</last></author>
      <author><first>Jian-Yun</first><last>Nie</last></author>
      <author><first>Laure</first><last>Soulier</last></author>
      <pages>37–58</pages>
      <abstract>Une des particularités des systèmes de recherche conversationnelle est qu’ils impliquent des initiatives mixtes telles que des questions de clarification des requêtes générées par le système pour mieux comprendre le besoin utilisateur. L’évaluation de ces systèmes à grande échelle sur la tâche finale de RI est très difficile et nécessite des ensembles de données adéquats contenant de telles interactions. Cependant, les jeux de données actuels se concentrent uniquement sur les tâches traditionnelles de RI ad hoc ou sur les tâches de clarification de la requête. Pour combler cette lacune, nous proposons une méthodologie pour construire automatiquement des ensembles de données de RI conversationnelle à grande échelle à partir d’ensembles de données de RI ad hoc afin de faciliter les explorations sur la RI conversationnelle. Nous effectuons une évaluation approfondie montrant la qualité et la pertinence des interactions générées pour chaque requête initiale. Cet article montre la faisabilité et l’utilité de l’augmentation des ensembles de données de RI ad-hoc pour la RI conversationnelle.</abstract>
      <url hash="3aeecf1a">2023.jeptalnrecital-coria.3</url>
      <language>fra</language>
      <bibkey>erbacher-etal-2023-augmentation</bibkey>
    </paper>
    <paper id="4">
      <title>Apprentissage de sous-espaces de préfixes</title>
      <author><first>Louis</first><last>Falissard</last></author>
      <author><first>Vincent</first><last>Guigue</last></author>
      <author><first>Laure</first><last>Soulier</last></author>
      <pages>59–73</pages>
      <abstract>Cet article propose une nouvelle façon d’ajuster des modèles de langue en “Few-shot learning” se basant sur une méthode d’optimisation récemment introduite en vision informatique, l’apprentissage de sous-espaces de modèles. Cette méthode, permettant de trouver non pas un point minimum local de la fonction coût dans l’espace des paramètres du modèle, mais tout un simplexe associé à des valeurs basses, présente typiquement des capacités de généralisation supérieures aux solutions obtenues par ajustement traditionnel. L’adaptation de cette méthode aux gros modèles de langue n’est pas triviale mais son application aux méthodes d’ajustement dites “Parameter Efficient” est quant à elle relativement naturelle. On propose de plus une façon innovante d’utiliser le simplexe de solution étudié afin de revisiter la notion de guidage de l’ajustement d’un modèle par l’inférence d’une métrique de validation, problématique d’actualité en “few-shot learning”. On montre finalement que ces différentes contributions centrées autour de l’ajustement de sous-espaces de modèles est empiriquement associée à un gain considérable en performances de généralisation sur les tâches de compréhension du langage du benchmark GLUE, dans un contexte de “few-shot learning”.</abstract>
      <url hash="63b2ee0a">2023.jeptalnrecital-coria.4</url>
      <language>fra</language>
      <bibkey>falissard-etal-2023-apprentissage</bibkey>
    </paper>
    <paper id="5">
      <title>Recherche cross-modale pour répondre à des questions visuelles</title>
      <author><first>Paul</first><last>Lerner</last></author>
      <author><first>Ferret</first><last>Olivier</last></author>
      <author><first>Camille</first><last>Guinaudeau</last></author>
      <pages>74–92</pages>
      <abstract>Répondre à des questions visuelles à propos d’entités nommées (KVQAE) est une tâche difficile qui demande de rechercher des informations dans une base de connaissances multimodale. Nous étudions ici comment traiter cette tâche avec une recherche cross-modale et sa combinaison avec une recherche mono-modale, en se focalisant sur le modèle CLIP, un modèle multimodal entraîné sur des images appareillées à leur légende textuelle. Nos résultats démontrent la supériorité de la recherche cross-modale, mais aussi la complémentarité des deux, qui peuvent être combinées facilement. Nous étudions également différentes manières d’ajuster CLIP et trouvons que l’optimisation cross-modale est la meilleure solution, étant en adéquation avec son pré-entraînement. Notre méthode surpasse les approches précédentes, tout en étant plus simple et moins coûteuse. Ces gains de performance sont étudiés intrinsèquement selon la pertinence des résultats de la recherche et extrinsèquement selon l’exactitude de la réponse extraite par un module externe. Nous discutons des différences entre ces métriques et de ses implications pour l’évaluation de la KVQAE.</abstract>
      <url hash="b6101ed9">2023.jeptalnrecital-coria.5</url>
      <language>fra</language>
      <bibkey>lerner-etal-2023-recherche</bibkey>
    </paper>
    <paper id="6">
      <title>Adaptation de domaine pour la recherche dense par annotation automatique</title>
      <author><first>Minghan</first><last>Li</last></author>
      <author><first>Eric</first><last>Gaussier</last></author>
      <pages>93–110</pages>
      <abstract>Bien que la recherche d’information neuronale ait connu des améliorations, les modèles de recherche dense ont une capacité de généralisation à de nouveaux domaines limitée, contrairement aux modèles basés sur l’interaction. Les approches d’apprentissage adversarial et de génération de requêtes n’ont pas résolu ce problème. Cet article propose une approche d’auto-supervision utilisant des étiquettes de pseudo-pertinence automatiquement générées pour le domaine cible. Le modèle T53B est utilisé pour réordonner une liste de documents fournie par BM25 afin d’obtenir une annotation des exemples positifs. L’extraction des exemples négatifs est effectuée en explorant différentes stratégies. Les expériences montrent que cette approche aide le modèle dense sur le domaine cible et améliore l’approche de génération de requêtes GPL.</abstract>
      <url hash="a1018d15">2023.jeptalnrecital-coria.6</url>
      <language>fra</language>
      <bibkey>li-gaussier-2023-adaptation</bibkey>
    </paper>
    <paper id="7">
      <title>Extraction d’entités nommées à partir de descriptions d’espèces</title>
      <author><first>Maya</first><last>Sahraoui</last></author>
      <author><first>Vincent</first><last>Guigue</last></author>
      <author><first>Régine</first><last>Vignes-Lebbe</last></author>
      <author><first>Marc</first><last>Pignal</last></author>
      <pages>111–126</pages>
      <abstract>Les descriptions d’espèces contiennent des informations importantes sur les caractéristiques morphologiques des espèces, mais l’extraction de connaissances structurées à partir de ces descriptions est souvent chronophage. Nous proposons un modèle texte-graphe adapté aux descriptions d’espèces en utilisant la reconnaissance d’entités nommées (NER) faiblement supervisée. Après avoir extrait les entités nommées, nous reconstruisons les triplets en utilisant des règles de dépendance pour créer le graphe. Notre méthode permet de comparer différentes espèces sur la base de caractères morphologiques et de relier différentes sources de données. Les résultats de notre étude se concentrent sur notre modèle NER et démontrent qu’il est plus performant que les modèles de référence et qu’il constitue un outil précieux pour la communauté de l’écologie et de la biodiversité.</abstract>
      <url hash="8817be75">2023.jeptalnrecital-coria.7</url>
      <language>fra</language>
      <bibkey>sahraoui-etal-2023-extraction</bibkey>
    </paper>
    <paper id="8">
      <title>Le théâtre français du <fixed-case>XVII</fixed-case>e siècle : une expérience en catégorisation de textes</title>
      <author><first>Jacques</first><last>Savoy</last></author>
      <pages>127–138</pages>
      <abstract>La catégorisation de documents (attribution d’un texte à une ou plusieurs catégories prédéfinies) possède de multiples applications. Cette communication se focalise sur l’attribution d’auteur en analysant le style de vingt pièces de théâtre du XVIIe siècle. L’hypothèse que nous souhaitons vérifier admet que le véritable auteur est le nom apparaissant sur la couverture. Afin de vérifier la qualité de deux méthodes d’attribution, nous avons repris deux corpus additionnels basés sur des romans écrits en français et italien. Nous proposons une amélioration de la méthode Delta ainsi qu’une nouvelle grille d’analyse pour cette approche. Ensuite, nous avons appliqué ces approches sur notre collection de comédies. Les résultats démontrent que l’hypothèse de base doit être écartée. De plus, ces œuvres présentent des styles proches rendant toute attribution difficile.</abstract>
      <url hash="b18d8cf5">2023.jeptalnrecital-coria.8</url>
      <language>fra</language>
      <bibkey>savoy-2023-le</bibkey>
    </paper>
    <paper id="9">
      <title>Enrichissement des modèles de langue pré-entraînés par la distillation mutuelle des connaissances</title>
      <author><first>Raphaël</first><last>Sourty</last></author>
      <author><first>Jose G</first><last>Moreno</last></author>
      <author><first>François-Paul</first><last>Servant</last></author>
      <author><first>Lynda</first><last>Tamine</last></author>
      <pages>139–156</pages>
      <abstract>Les bases de connaissances sont des ressources essentielles dans un large éventail d’applications à forte intensité de connaissances. Cependant, leur incomplétude limite intrinsèquement leur utilisation et souligne l’importance de les compléter. À cette fin, la littérature a récemment adopté un point de vue de monde ouvert en associant la capacité des bases de connaissances à représenter des connaissances factuelles aux capacités des modèles de langage pré-entraînés (PLM) à capturer des connaissances linguistiques de haut niveau et contextuelles à partir de corpus de textes. Dans ce travail, nous proposons un cadre de distillation pour la complétion des bases de connaissances où les PLMs exploitent les étiquettes souples sous la forme de prédictions d’entités et de relations fournies par un modèle de plongements de bases de connaissances, tout en conservant leur pouvoir de prédiction d’entités sur de grandes collections des textes. Pour mieux s’adapter à la tâche de complétion des connaissances, nous étendons la modélisation traditionnelle du langage masqué des PLM à la prédiction d’entités et d’entités liées dans le contexte. Des expériences utilisant les tâches à forte intensité de connaissances dans le cadre du benchmark d’évaluation KILT montrent le potentiel de notre approche.</abstract>
      <url hash="2323fe21">2023.jeptalnrecital-coria.9</url>
      <language>fra</language>
      <bibkey>sourty-etal-2023-enrichissement</bibkey>
    </paper>
    <paper id="10">
      <title>Constitution de sous-fils de conversations d’emails</title>
      <author><first>Lionel</first><last>Tadonfouet Tadjou</last></author>
      <author><first>Eric</first><last>De La Clergerie</last></author>
      <author><first>Fabrice</first><last>Bourge</last></author>
      <author><first>Tiphaine</first><last>Marie</last></author>
      <pages>157–171</pages>
      <abstract>Les conversations d’emails en entreprise sont parfois difficiles à suivre par les collaborateurs car elles peuvent traiter de plusieurs sujets à la fois et impliquer de nombreux interlocuteurs. Pour faciliter la compréhension des messages clés, il est utile de créer des sous-fils de conversations. Dans notre étude, nous proposons un pipeline en deux étapes pour reconnaître les actes de dialogue dans les segments de texte d’une conversation et les relier pour améliorer l’accessibilité de l’information. Ce pipeline construit ainsi des paires de segments de texte transverses sur les emails d’une conversationfacilitant ainsi la compréhension des messages clés inhérents à celle-ci. A notre connaissance, c’est la première fois que cette problématique de constitution de fils de conversations est abordée sur les conversations d’emails. Nous avons annoté le corpus d’emails BC3 en actes de dialogues et mis enrelation les segments de texte de conversation d’emails de BC3.</abstract>
      <url hash="9552bf01">2023.jeptalnrecital-coria.10</url>
      <language>fra</language>
      <bibkey>tadonfouet-tadjou-etal-2023-constitution</bibkey>
    </paper>
    <paper id="11">
      <title>Intégration du raisonnement numérique dans les modèles de langue : État de l’art et direction de recherche</title>
      <author><first>Sarah</first><last>Abchiche</last></author>
      <author><first>Lynda</first><last>Said Lhadj</last></author>
      <author><first>Vincent</first><last>Guigue</last></author>
      <author><first>Laure</first><last>Soulier</last></author>
      <pages>173–184</pages>
      <abstract>Ces dernières années, les modèles de langue ont connu une évolution galopante grâce à l’augmentation de la puissance de calcul qui a rendu possible l’utilisation des réseaux de neurones. Parallèlement, l’intégration du raisonnement numérique dans les modèles de langue a suscité un intérêt grandissant. Pourtant, bien que l’entraînement des modèles de langue sur des données numériques soit devenu un paradigme courant, les modèles actuels ne parviennent pas à effectuer des calculs de manière satisfaisante. Pour y remédier, une solution est d’entraîner les modèles de langue à utiliser des outils externes tels qu’une calculatrice ou un “runtime” de code python pour effectuer le raisonnement numérique. L’objectif de ce papier est double, dans un premier temps nous passons en revue les travaux de l’état de l’art sur le raisonnement numérique dans les modèles de langue et dans un second temps nous discutons des différentes perspectives de recherche pour augmenter les compétences numériques des modèles.</abstract>
      <url hash="84894bd1">2023.jeptalnrecital-coria.11</url>
      <language>fra</language>
      <bibkey>abchiche-etal-2023-integration</bibkey>
    </paper>
    <paper id="12">
      <title>Reconnaissance d’Entités Nommées fondée sur des Modèles de Langue Enrichis avec des Définitions des Types d’Entités</title>
      <author><first>Jesús</first><last>Lovón Melgarejo</last></author>
      <author><first>Jose</first><last>Moreno</last></author>
      <author><first>Romaric</first><last>Besançon</last></author>
      <author><first>Olivier</first><last>Ferret</last></author>
      <author><first>Lynda</first><last>Tamine</last></author>
      <pages>185–194</pages>
      <abstract>Des études récentes ont identifié de nouveaux défis dans la tâche de reconnaissance d’entités nommées (NER), tels que la reconnaissance d’entités complexes qui ne sont pas des phrases nominales simples et/ou figurent dans des entrées textuelles courtes, avec une faible quantité d’informations contextuelles. Cet article propose une nouvelle approche qui relève ce défi, en se basant sur des modèles de langues pré-entraînés par enrichissement des définitions des types d’entités issus d’une base de connaissances. Les expériences menées dans le cadre de la tâche MultiCoNER I de SemEval ont montré que l’approche proposée permet d’atteindre des gains en performance par rapport aux modèles de référence de la tâche.</abstract>
      <url hash="b743ee36">2023.jeptalnrecital-coria.12</url>
      <language>fra</language>
      <bibkey>lovon-melgarejo-etal-2023-reconnaissance</bibkey>
    </paper>
    <paper id="13">
      <title>Entity Enhanced Attention Graph-Based Passages Retrieval</title>
      <author><first>Lucas</first><last>Albarede</last></author>
      <author><first>Lorraine</first><last>Goeuriot</last></author>
      <author><first>Philippe</first><last>Mulhem</last></author>
      <author><first>Claude</first><last>Le Pape-Gardeux</last></author>
      <author><first>Sylvain</first><last>Marie</last></author>
      <author><first>Trinidad</first><last>Chardin-Segui</last></author>
      <pages>196–200</pages>
      <abstract>Passage retrieval is crucial in specialized domains where documents are long and complex, such as patents, legal documents, scientific reports, etc. We explore in this paper the integration of Entities and passages in Heterogeneous Attention Graph Models dedicated to passage retrieval. We use the two passage retrieval architectures based on re-ranking proposed in [1]. We experiment our proposal on the TREC CAR Y3 Passage Retrieval Task. The results obtained show an improvement over state-of-the-art techniques and proves the effectiveness of the approach. Our experiments also show the importance of using adequate parameters for such approach.</abstract>
      <url hash="2a56f989">2023.jeptalnrecital-coria.13</url>
      <bibkey>albarede-etal-2023-entity</bibkey>
    </paper>
    <paper id="14">
      <title>Highlighting exact matching via marking strategies for ad hoc document ranking with pretrained contextualized language models</title>
      <author><first>Lila</first><last>Boualili</last></author>
      <author><first>Jose</first><last>Moreno</last></author>
      <author><first>Mohand</first><last>Boughanem</last></author>
      <pages>201–201</pages>
      <abstract>Les modèles de langue pré-entraînés (MLPs) à l’instar de BERT se sont révélés remarquablement efficaces pour le classement ad hoc. Contrairement aux modèles antérieurs à BERT qui nécessitent des composants neuronaux spécialisés pour capturer les différents aspects de la pertinence entre la requête et le document, les MLPs sont uniquement basés sur des blocs de “transformers” où l’attention est le seul mécanisme utilisé pour extraire des signaux à partir des interactions entre les termes de la requête et le document. Grâce à l’attention croisée du “transformer”, BERT s’est avéré être un modèle d’appariement sémantique efficace. Cependant, l’appariement exact reste un signal essentiel pour évaluer la pertinence d’un document par rapport à une requête de recherche d’informations, en dehors de l’appariement sémantique. Dans cet article, nous partons de l’hypothèse que BERT pourrait bénéficier d’indices explicites d’appariement exact pour mieux s’adapter à la tâche d’estimation de pertinence. Dans ce travail, nous explorons des stratégies d’intégration des signaux d’appariement exact en utilisant des “tokens” de marquage permettant de mettre en évidence les correspondances exactes entre les termes de la requête et ceux du document. Nous constatons que cette approche de marquage simple améliore de manière significative le modèle BERT vanille de référence. Nous démontrons empiriquement l’efficacité de notre approche par le biais d’expériences exhaustives sur trois collections standards en recherche d’information (RI). Les résultats montrent que les indices explicites de correspondance exacte transmis par le marquage sont bénéfiques pour des MLPs aussi bien BERT que pour ELECTRA. Nos résultats confirment que les indices traditionnels de RI, tels que la correspondance exacte de termes, sont toujours utiles pour les nouveaux modèles contextualisés pré-entraînés tels que BERT.</abstract>
      <url hash="f31af2c8">2023.jeptalnrecital-coria.14</url>
      <bibkey>boualili-etal-2023-highlighting</bibkey>
    </paper>
    <paper id="15">
      <title>Vers l’évaluation continue des systèmes de recherche d’information.</title>
      <author><first>Petra</first><last>Galuscakova</last></author>
      <author><first>Romain</first><last>Deveaud</last></author>
      <author><first>Gabriela</first><last>Gonzalez-Saez</last></author>
      <author><first>Philippe</first><last>Mulhem</last></author>
      <author><first>Lorraine</first><last>Goeuriot</last></author>
      <author><first>Florina</first><last>Piroi</last></author>
      <author><first>Martin</first><last>Popel</last></author>
      <pages>202–206</pages>
      <abstract>Cet article présente le corpus de données associé à la première campagne évaluation LongEval dans le cadre de CLEF 2023. L’objectif de cette évaluation est d’étudier comment les systèmes de recherche d’informations réagissent à l’évolution des données qu’ils manipulent (notamment les documents et les requêtes). Nous détaillons les objectifs de la tâche, le processus d’acquisition des données et les mesures d’évaluation utilisées.</abstract>
      <url hash="344e039f">2023.jeptalnrecital-coria.15</url>
      <language>fra</language>
      <bibkey>galuscakova-etal-2023-vers</bibkey>
    </paper>
    <paper id="16">
      <title><fixed-case>C</fixed-case>o<fixed-case>SPLADE</fixed-case> : Adaptation d’un Modèle Neuronal Basé sur des Représentations Parcimonieuses pour la Recherche d’Information Conversationnelle</title>
      <author><first>Nam</first><last>Le Hai</last></author>
      <author><first>Thomas</first><last>Gerald</last></author>
      <author><first>Thibault</first><last>Formal</last></author>
      <author><first>Jian-Yun</first><last>Nie</last></author>
      <author><first>Benjamin</first><last>Piwowarksi</last></author>
      <author><first>Laure</first><last>Soulier</last></author>
      <pages>207–212</pages>
      <abstract>La recherche conversationnelle est une tâche qui vise à retrouver des documents à partir de la questioncourante de l’utilisateur ainsi que l’historique complet de la conversation. La plupart des méthodesantérieures sont basées sur une approche multi-étapes reposant sur une reformulation de la question.Cette étape de reformulation est critique, car elle peut conduire à un classement sous-optimal des do-cuments. D’autres approches ont essayé d’ordonner directement les documents, mais s’appuient pourla plupart sur un jeu de données contenant des pseudo-labels. Dans ce travail, nous proposons une tech-nique d’apprentissage à la fois “légère” et innovante pour un modèle contextualisé d’ordonnancementbasé sur SPLADE. En s’appuyant sur les représentations parcimonieuses de SPLADE, nous montronsque notre modèle, lorsqu’il est combiné avec le modèle de ré-ordonnancement T5Mono, obtient desrésultats qui sont compétitifs avec ceux obtenus par les participants des campagnes d’évaluation TRECCAsT 2020 et 2021. Le code source est disponible sur https://github.com/anonymous.</abstract>
      <url hash="76554917">2023.jeptalnrecital-coria.16</url>
      <language>fra</language>
      <bibkey>le-hai-etal-2023-cosplade</bibkey>
    </paper>
    <paper id="17">
      <title>The Power of Selecting Key Blocks with Local Pre-ranking for Long Document Information Retrieval</title>
      <author><first>Minghan</first><last>Li</last></author>
      <author><first>Diana Nicoleta</first><last>Popa</last></author>
      <author><first>Johan</first><last>Chagnon</last></author>
      <author><first>Yagmur Gizem</first><last>Cinar</last></author>
      <author><first>Eric</first><last>Gaussier</last></author>
      <pages>213–213</pages>
      <abstract>Les réseaux neuronaux profonds et les modèles fondés sur les transformeurs comme BERT ont envahi le domaine de la recherche d’informations (RI) ces dernières années. Leur succès est lié au mécanisme d’auto-attention qui permet de capturer les dépendances entre les mots indépendamment de leur distance. Cependant, en raison de sa complexité quadratique dans le nombre de mots, ce mécanisme ne peut être directement utilisé sur de longues séquences, ce qui ne permet pas de déployer entièrement les modèles neuronaux sur des documents longs pouvant contenir des milliers de mots. Trois stratégies standard ont été adoptées pour contourner ce problème. La première consiste à tronquer les documents longs, la deuxième à segmenter les documents longs en passages plus courts et la dernière à remplacer le module d’auto-attention par des modules d’attention parcimonieux. Dans le premier cas, des informations importantes peuvent être perdues et le jugement de pertinence n’est fondé que sur une partie de l’information contenue dans le document. Dans le deuxième cas, une architecture hiérarchique peut être adoptée pour construire une représentation du document sur la base des représentations de chaque passage. Cela dit, malgré ses résultats prometteurs, cette stratégie reste coûteuse en temps, en mémoire et en énergie. Dans le troisième cas, les contraintes de parcimonie peuvent conduire à manquer des dépendances importantes et, in fine, à des résultats sous-optimaux. L’approche que nous proposons est légèrement différente de ces stratégies et vise à capturer, dans les documents longs, les blocs les plus importants permettant de décider du statut, pertinent ou non, de l’ensemble du document. Elle repose sur trois étapes principales : (a) la sélection de blocs clés (c’est-à-dire susceptibles d’être pertinents) avec un pré-classement local en utilisant soit des modèles de RI classiques, soit un module d’apprentissage, (b) l’apprentissage d’une représentation conjointe des requêtes et des blocs clés à l’aide d’un modèle BERT standard, et (c) le calcul d’un score de pertinence final qui peut être considéré comme une agrégation d’informations de pertinence locale. Dans cet article, nous menons tout d’abord une analyse qui révèle que les signaux de pertinence peuvent apparaître à différents endroits dans les documents et que de tels signaux sont mieux capturés par des relations sémantiques que par des correspondances exactes. Nous examinons ensuite plusieurs méthodes pour sélectionner les blocs pertinents et montrons comment intégrer ces méthodes dans les modèles récents de RI.</abstract>
      <url hash="43d3316e">2023.jeptalnrecital-coria.17</url>
      <bibkey>li-etal-2023-power</bibkey>
    </paper>
    <paper id="18">
      <title>i<fixed-case>QPP</fixed-case>: Une Référence pour la Prédiction de Performances des Requêtes d’Images</title>
      <author><first>Eduard</first><last>Poesina</last></author>
      <author><first>Radu Tudor</first><last>Ionescu</last></author>
      <author><first>Josiane</first><last>Mothe</last></author>
      <pages>214–220</pages>
      <abstract>La prédiction de la performance des requêtes (QPP) dans le contexte de la recherche d’images basée sur le contenu reste une tâche largement inexplorée, en particulier dans le scénario de la recherche par l’exemple, où la requête est une image. Pour stimuler les recherches dans ce domaine, nous proposons la première collection de référence. Nous proposons un ensemble de quatre jeux de données (PASCAL VOC 2012, Caltech-101, ROxford5k et RParis6k) avec les performances attendues pour chaque requête à l’aide de deux modèles de recherche d’images état de l’art. Nous proposons également de nouveaux prédicteurs pré et post-recherche. Les résultats empiriques montrent que la plupart des prédicteurs ne se généralisent pas aux différents scénarios d’évaluation. Nos expériences exhaustives indiquent que l’iQPP est une référence difficile, révélant une importante lacune dans la recherche qui doit être abordée dans les travaux futurs. Nous publions notre code et nos données.</abstract>
      <url hash="00b98e85">2023.jeptalnrecital-coria.18</url>
      <language>fra</language>
      <bibkey>poesina-etal-2023-iqpp</bibkey>
    </paper>
    <paper id="19">
      <title><fixed-case>XPMIR</fixed-case>: Une bibliothèque modulaire pour l’apprentissage d’ordonnancement et les expériences de <fixed-case>RI</fixed-case> neuronale</title>
      <author><first>Yuxuan</first><last>Zong</last></author>
      <author><first>Benjamin</first><last>Piwowarski</last></author>
      <pages>222–233</pages>
      <abstract>Ces dernières années, plusieurs librairies pour la recherche d’information (neuronale) ont été proposées. Cependant, bien qu’elles permettent de reproduire des résultats déjà publiés, il est encore très difficile de réutiliser certaines parties des chaînes de traitement d’apprentissage, comme par exemple le pré-entraînement, la stratégie d’échantillonnage ou la définition du coût dans les modèles nouvellement développés. Il est également difficile d’utiliser de nouvelles techniques d’apprentissage avec d’anciens modèles, ce qui complique l’évaluation de l’utilité des nouvelles idées pour les différents modèles de RI neuronaux. Cela ralentit l’adoption de nouvelles techniques et, par conséquent, le développement du domaine de la RI. Dans cet article, nous présentons XPMIR, une librairie Python définissant un ensemble réutilisable de composants expérimentaux. La bibliothèque contient déjà des modèles et des techniques d’indexation de pointe, et est intégrée au hub HuggingFace.</abstract>
      <url hash="1ee7b885">2023.jeptalnrecital-coria.19</url>
      <language>fra</language>
      <bibkey>zong-piwowarski-2023-xpmir</bibkey>
    </paper>
  </volume>
  <volume id="rjc" ingest-date="2023-06-25">
    <meta>
      <booktitle>Actes de CORIA-TALN 2023. Actes des 16e Rencontres Jeunes Chercheurs en RI (RJCRI) et 25e Rencontre des Étudiants Chercheurs \\ en Informatique pour le Traitement Automatique des Langues (RÉCITAL)</booktitle>
      <editor><first>Marie</first><last>Candito</last></editor>
      <editor><first>Thomas</first><last>Gerald</last></editor>
      <editor><first>José G</first><last>Moreno</last></editor>
      <publisher>ATALA</publisher>
      <address>Paris, France</address>
      <month>6</month>
      <year>2023</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="67f0e382">2023.jeptalnrecital-rjc.0</url>
      <bibkey>jep-taln-recital-2023-actes-de</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Les jeux de données en compréhension du langage naturel et parlé : paradigmes d’annotation et représentations sémantiques</title>
      <author><first>Rim</first><last>Abrougui</last></author>
      <pages>1–20</pages>
      <abstract>La compréhension du langage naturel et parlé (NLU/SLU) couvre le problème d’extraire et d’annoter la structure sémantique, à partir des énoncés des utilisateurs dans le contexte des interactions humain/machine, telles que les systèmes de dialogue. Elle se compose souvent de deux tâches principales : la détection des intentions et la classification des concepts. Dans cet article, différents corpora SLU sont étudiés au niveau formel et sémantique : leurs différents formats d’annotations (à plat et structuré) et leurs ontologies ont été comparés et discutés. Avec leur pouvoir expressif gardant la hiérarchie sémantique entre les intentions et les concepts, les représentations sémantiques structurées sous forme de graphe ont été mises en exergue. En se positionnant vis à vis de la littérature et pour les futures études, une projection sémantique et une modification au niveau de l’ontologie du corpus MultiWOZ ont été proposées.</abstract>
      <url hash="ea655370">2023.jeptalnrecital-rjc.1</url>
      <language>fra</language>
      <bibkey>abrougui-2023-les</bibkey>
    </paper>
    <paper id="2">
      <title>Étude de la fidélité des entités dans les résumés par abstraction</title>
      <author><first>Eunice</first><last>Akani</last></author>
      <pages>21–36</pages>
      <abstract>L’un des problèmes majeurs dans le résumé automatique de texte par abstraction est la fidélité du résumé généré vis-à-vis du document. Les systèmes peuvent produire des informations incohérentes vis-à-vis du document. Ici, nous mettons l’accent sur ce phénomène en restant focalisé sur les entités nommées. L’objectif est de réduire les hallucinations sur celles-ci. Ainsi, nous avons généré des résumés par sampling et avons sélectionné, à l’aide d’un critère basé sur le risque d’hallucination sur les entités et les performances du modèle, ceux qui minimisent les hallucinations sur les entités. Une étude empirique du critère montre son adaptabilité pour la sélection de résumé. Nous avons proposé des heuristiques pour la détection des entités qui sont des variations ou flexions d’autres entités. Les résultats obtenus montrent que le critère réduit les hallucinations sur les entités nommées en gardant un score ROUGE comparable pour CNN/DM.</abstract>
      <url hash="878b64b7">2023.jeptalnrecital-rjc.2</url>
      <language>fra</language>
      <bibkey>akani-2023-etude</bibkey>
    </paper>
    <paper id="3">
      <title>Mise en place d’un modèle compact à architecture Transformer pour la détection jointe des intentions et des concepts dans le cadre d’un système interactif de questions-réponses</title>
      <author><first>Nadège</first><last>Alavoine</last></author>
      <author><first>Arthur</first><last>Babin</last></author>
      <pages>37–56</pages>
      <abstract>Les tâches de détection d’intention et d’identification des concepts sont toutes deux des éléments importants de la compréhension de la parole. Elles sont souvent réalisées par deux modules différents au sein d’un pipeline. L’apparition de modèles réalisant conjointement ces deux tâches a permis d’exploiter les dépendances entre elles et d’améliorer les performances obtenues. Plus récemment, des modèles de détection jointe reposant sur des architectures Transformer ont été décrits dans la littérature. Par ailleurs, avec la popularité et taille croissante des modèles Transformer ainsi que les inquiétudes ergonomiques et écologiques grandissantes, des modèles compacts ont été proposés. Dans cet article, nous présentons la mise en place et l’évaluation d’un modèle compact pour la détection jointe de l’intention et des concepts. Notre contexte applicatif est celui d’un système interactif de questions-réponses français.</abstract>
      <url hash="5573a1e0">2023.jeptalnrecital-rjc.3</url>
      <language>fra</language>
      <bibkey>alavoine-babin-2023-mise</bibkey>
    </paper>
    <paper id="4">
      <title>Utiliser les syntagmes nominaux complexes anglais pour évaluer la robustesse des systèmes de traduction anglais-français en langue de spécialité</title>
      <author><first>Maud</first><last>Bénard</last></author>
      <pages>57–71</pages>
      <abstract>Nous défendons l’idée que l’analyse des erreurs faites lors de la traduction des syntagmes nominaux complexes présente un intérêt pour évaluer la robustesse des systèmes de traduction automatique anglais-français en langue de spécialité. Ces constructions syntaxiques impliquent des questions de syntaxe et de lexique qui constituent un obstacle important à leur compréhension et leur production pour les locuteurs d’anglais non natifs. Nous soutenons que ces analyses contribueraient à garantir que les systèmes de TA répondent aux exigences linguistiques des utilisateurs finaux auxquels ils sont destinés.</abstract>
      <url hash="31c45ca1">2023.jeptalnrecital-rjc.4</url>
      <language>fra</language>
      <bibkey>benard-2023-utiliser</bibkey>
    </paper>
    <paper id="5">
      <title>Vers une implémentation de la théorie sens-texte avec les grammaires catégorielles abstraites</title>
      <author><first>Marie</first><last>Cousin</last></author>
      <pages>72–86</pages>
      <abstract>La théorie sens-texte est une théorie linguistique visant à décrire la correspondance entre le sens et le texte d’un énoncé à l’aide d’un outil formel qui simule l’activité langagière d’un locuteur natif. Nous avons mis en place une premiere implémentation de cette théorie à l’aide des grammaires catégorielles abstraites, qui sont un formalisme grammatical basé sur le lambda-calcul. Cette implémentation représente les trois niveaux de représentation sémantique, syntaxique profonde et syntaxique de surface de la théorie sens-texte. Elle montre que la transition de l’un à l’autre de ces niveaux (en particulier la génération d’une représentation de syntaxe de surface à partir d’une représentation sémantique d’un même énoncé) peut être implémentée en utilisant les propriétés avantageuses des grammaires catégorielles abstraites, dont la transduction.</abstract>
      <url hash="21ac028c">2023.jeptalnrecital-rjc.5</url>
      <language>fra</language>
      <bibkey>cousin-2023-vers</bibkey>
    </paper>
    <paper id="6">
      <title>Analyse de la légitimité des start-ups</title>
      <author><first>Asmaa</first><last>Lagrid</last></author>
      <pages>87–100</pages>
      <abstract>La légitimité est un élément crucial pour la stabilité et la survie des startups en phase de croissance. Ce concept est défini dans la littérature comme étant la perception de l’adéquation d’une organisation à un système social en termes de règles, valeurs, normes et définitions. En d’autres termes, la légitimité des startups repose sur l’alignement des jugements subjectifs avec les jugements objectifs des experts, basés sur les performances des startups. Cette mesure de la subjectivité de la légitimité est très similaire à l’analyse des sentiments financiers réalisée sur les entreprises pour évaluer leur santé financière et prendre des décisions d’investissement. Dans ce travail, nous présentons les travaux sur la légitimité et les avancées de l’analyse des sentiments qui peuvent nous aider à analyser la légitimité. Nous examinons également les similitudes et les différences entre la légitimité et l’analyse des sentiments financiers. Nous présentons une première expérimentation sur les annonces de projets sur une plateforme de crowdfunding, en utilisant le modèle DistilBERT, qui a déjà été largement utilisé pour la classification de texte. En conclusion, nous discutons des perspectives de notre recherche pour mesurer la légitimité des startups.</abstract>
      <url hash="6b752481">2023.jeptalnrecital-rjc.6</url>
      <language>fra</language>
      <bibkey>lagrid-2023-analyse</bibkey>
    </paper>
    <paper id="7">
      <title>Approches neuronales pour la détection des chaînes de coréférences : un état de l’art</title>
      <author><first>Fabien</first><last>Lopez</last></author>
      <pages>101–113</pages>
      <abstract>La résolution des liens de coréférences est une tâche importante du TALN impliquant cohérence et compréhension d’un texte. Nous présenterons dans ce papier une vision actuelle de l’état de l’art sur la résolution des liens de coréférence depuis 2013 et l’avènement des modèles neuronaux pour cette tâche. Cela comprend les corpus disponibles en français, les méthodes d’évaluation ainsi que les différentes architectures et leur approche. Enfin nous détaillerons les résultats, témoignant de l’évolution de méthodes de résolutions des liens de coréférences.</abstract>
      <url hash="56498cc4">2023.jeptalnrecital-rjc.7</url>
      <language>fra</language>
      <bibkey>lopez-2023-approches</bibkey>
    </paper>
    <paper id="8">
      <title>Etudes sur la géolocalisation de Tweets</title>
      <author><first>Thibaud</first><last>Martin</last></author>
      <pages>114–130</pages>
      <abstract>La géolocalisation de textes non structurés est un problème de recherche consistant à extraire uncontexte géographique d’un texte court. Sa résolution passe typiquement par une recherche de termesspatiaux et de la désambiguïsation. Dans cet article, nous proposons une analyse du problème, ainsi que deux méthodes d’inférence pourdéterminer le lieu dont traite un texte : 1. Comparaison de termes spatiaux à un index géographique2. Géolocalisation de textes sans information géographique à partir d’un graphe de co-occurrencede termes (avec et sans composante temporelle) Nos recherches sont basées sur un dataset de 10 millions de Tweets traitant de lieux français, dont57 830 possèdent une coordonnée géographique.</abstract>
      <url hash="4099995c">2023.jeptalnrecital-rjc.8</url>
      <language>fra</language>
      <bibkey>martin-2023-etudes</bibkey>
    </paper>
    <paper id="9">
      <title><fixed-case>IR</fixed-case>-<fixed-case>S</fixed-case>en<fixed-case>T</fixed-case>rans<fixed-case>B</fixed-case>io: Modèles Neuronaux Siamois pour la Recherche d’Information Biomédicale</title>
      <author><first>Safaa</first><last>Menad</last></author>
      <pages>131–142</pages>
      <abstract>L’entraînement de modèles transformeurs de langages sur des données biomédicales a permis d’obtenir des résultats prometteurs. Cependant, ces modèles de langage nécessitent pour chaque tâche un affinement (fine-tuning) sur des données supervisées très spécifiques qui sont peu disponibles dans le domaine biomédical. Dans le cadre de la classification d’articles scientifiques et les réponses aux questions biomédicales, nous proposons d’utiliser de nouveaux modèles neuronaux siamois (sentence transformers) qui plongent des textes à comparer dans un espace vectoriel. Nos modèles optimisent une fonction objectif d’apprentissage contrastif auto-supervisé sur des articles issus de la base de données bibliographique MEDLINE associés à leurs mots-clés MeSH (Medical Subject Headings). Les résultats obtenus sur plusieurs benchmarks montrent que les modèles proposés permettent de résoudre ces tâches sans exemples (zero-shot) et sont comparables à des modèles transformeurs biomédicaux affinés sur des données supervisés spécifiques aux problèmes traités. De plus, nous exploitons nos modèles dans la tâche de la recherche d’information biomédicale. Nous montrons que la combinaison de la méthode BM25 et de nos modèles permet d’obtenir des améliorations supplémentaires dans ce cadre.</abstract>
      <url hash="27e38187">2023.jeptalnrecital-rjc.9</url>
      <language>fra</language>
      <bibkey>menad-2023-ir</bibkey>
    </paper>
    <paper id="10">
      <title>L’évaluation de la traduction automatique du caractère au document : un état de l’art</title>
      <author><first>Mariam</first><last>Nakhlé</last></author>
      <pages>143–159</pages>
      <abstract>Ces dernières années l’évaluation de la traduction automatique, qu’elle soit humaine ou automatique,a rencontré des difficultés. Face aux importantes avancées en matière de traduction automatiqueneuronale, l’évaluation s’est montrée peu fiable. De nombreuses nouvelles approches ont été pro-posées pour améliorer les protocoles d’évaluation. L’objectif de ce travail est de proposer une vued’ensemble sur l’état global de l’évaluation de la Traduction Automatique (TA). Nous commenceronspar exposer les approches d’évaluation humaine, ensuite nous présenterons les méthodes d’évaluationautomatiques tout en différenciant entre les familles d’approches (métriques superficielles et apprises)et nous prêterons une attention particulière à l’évaluation au niveau du document qui prend comptedu contexte. Pour terminer, nous nous concentrerons sur la méta-évaluation des méthodes.</abstract>
      <url hash="4dc49e75">2023.jeptalnrecital-rjc.10</url>
      <language>fra</language>
      <bibkey>nakhle-2023-levaluation</bibkey>
    </paper>
    <paper id="11">
      <title>Normalisation lexicale de contenus générés par les utilisateurs sur les réseaux sociaux</title>
      <author><first>Lydia</first><last>Nishimwe</last></author>
      <pages>160–183</pages>
      <abstract>L’essor du traitement automatique des langues (TAL) se vit dans un monde où l’on produit de plus en plus de contenus en ligne. En particulier sur les réseaux sociaux, les textes publiés par les internautes sont remplis de phénomènes « non standards » tels que les fautes d’orthographe, l’argot, les marques d’expressivité, etc. Ainsi, les modèles de TAL, en grande partie entraînés sur des données « standards », voient leur performance diminuer lorsqu’ils sont appliqués aux contenus générés par les utilisateurs (CGU). L’une des approches pour atténuer cette dégradation est la normalisation lexicale : les mots non standards sont remplacés par leurs formes standards. Dans cet article, nous réalisons un état de l’art de la normalisation lexicale des CGU, ainsi qu’une étude expérimentale préliminaire pour montrer les avantages et les difficultés de cette tâche.</abstract>
      <url hash="76606a4b">2023.jeptalnrecital-rjc.11</url>
      <language>fra</language>
      <bibkey>nishimwe-2023-normalisation</bibkey>
    </paper>
  </volume>
</collection>
