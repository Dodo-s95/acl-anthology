<?xml version='1.0' encoding='UTF-8'?>
<collection id="2023.jeptalnrecital">
  <volume id="coria" ingest-date="2023-06-25">
    <meta>
      <booktitle>Actes de CORIA-TALN 2023. Actes de la 18e Conférence en Recherche d'Information et Applications (CORIA)</booktitle>
      <editor><first>Haïfa</first><last>Zargayouna</last></editor>
      <publisher>ATALA</publisher>
      <address>Paris, France</address>
      <month>6</month>
      <year>2023</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="50d313b5">2023.jeptalnrecital-coria.0</url>
      <bibkey>jep-taln-recital-2023-actes</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Impact de l’apprentissage multi-labels actif appliqué aux transformers</title>
      <author><first>Maxime</first><last>Arens</last></author>
      <author><first>Charles</first><last>Teissèdre</last></author>
      <author><first>Lucile</first><last>Callebert</last></author>
      <author><first>Jose G</first><last>Moreno</last></author>
      <author><first>Mohand</first><last>Boughanem</last></author>
      <pages>2–17</pages>
      <abstract>L’Apprentissage Actif (AA) est largement utilisé en apprentissage automatique afin de réduire l’effort d’annotation. Bien que la plupart des travaux d’AA soient antérieurs aux transformers, le succès récent de ces architectures a conduit la communauté à revisiter l’AA dans le contexte des modèles de langues pré-entraînés.De plus, le mécanisme de fine-tuning, où seules quelques données annotées sont utilisées pour entraîner le modèle sur une nouvelle tâche, est parfaitement en accord avec l’objectif de l’AA. Nous proposons d’étudier l’impact de l’AA dans le contexte des transformers pour la tâche de classification multi-labels. Or la plupart des stratégies AA, lorsqu’elles sont appliquées à ces modèles, conduisent à des temps de calcul excessifs, ce qui empêche leur utilisation au cours d’une interaction homme-machine en temps réel. Afin de pallier ce problème, nous utilisons des stratégies d’AA basées sur l’incertitude. L’article compare six stratégies d’AA basées sur l’incertitude dans le contexte des transformers et montre que si deux stratégies améliorent invariablement les performances, les autres ne surpassent pas l’échantillonnage aléatoire. L’étude montre également que les stratégies performantes ont tendance à sélectionner des ensembles d’instances plus diversifiées pour l’annotation.</abstract>
      <url hash="58e2263e">2023.jeptalnrecital-coria.1</url>
      <language>fra</language>
      <bibkey>arens-etal-2023-impact</bibkey>
    </paper>
    <paper id="2">
      <title>Quelles évolutions sur cette loi ? Entre abstraction et hallucination dans le domaine du résumé de textes juridiques</title>
      <author><first>Nihed</first><last>Bendahman</last></author>
      <author><first>Karen</first><last>Pinel-Sauvagnat</last></author>
      <author><first>Gilles</first><last>Hubert</last></author>
      <author><first>Mokhtar Boumedyen</first><last>Billami</last></author>
      <pages>18–36</pages>
      <abstract>Résumer automatiquement des textes juridiques permettrait aux chargés de veille d’éviter une surcharge informationnelle et de gagner du temps sur une activité particulièrement chronophage. Dans cet article, nous présentons un corpus de textes juridiques en français associés à des résumés de référence produits par des experts, et cherchons à établir quels modèles génératifs de résumé sont les plus intéressants sur ces documents possédant de fortes spécificités métier. Nous étudions quatre modèles de l’état de l’art, que nous commençons à évaluer avec des métriques traditionnelles. Afin de comprendre en détail la capacité des modèles à transcrire les spécificités métiers, nous effectuons une analyse plus fine sur les entités d’intérêt. Nous évaluons notamment la couverture des résumés en termes d’entités, mais aussi l’apparition d’informations non présentes dans les documents d’origine, dites hallucinations. Les premiers résultats montrent que le contrôle des hallucinations est crucial dans les domaines de spécialité, particulièrement le juridique.</abstract>
      <url hash="c0917a39">2023.jeptalnrecital-coria.2</url>
      <language>fra</language>
      <bibkey>bendahman-etal-2023-quelles</bibkey>
    </paper>
    <paper id="3">
      <title>Augmentation de jeux de données <fixed-case>RI</fixed-case> pour la recherche conversationnelle à initiative mixte</title>
      <author><first>Pierre</first><last>Erbacher</last></author>
      <author><first>Philippe</first><last>Preux</last></author>
      <author><first>Jian-Yun</first><last>Nie</last></author>
      <author><first>Laure</first><last>Soulier</last></author>
      <pages>37–58</pages>
      <abstract>Une des particularités des systèmes de recherche conversationnelle est qu’ils impliquent des initiatives mixtes telles que des questions de clarification des requêtes générées par le système pour mieux comprendre le besoin utilisateur. L’évaluation de ces systèmes à grande échelle sur la tâche finale de RI est très difficile et nécessite des ensembles de données adéquats contenant de telles interactions. Cependant, les jeux de données actuels se concentrent uniquement sur les tâches traditionnelles de RI ad hoc ou sur les tâches de clarification de la requête. Pour combler cette lacune, nous proposons une méthodologie pour construire automatiquement des ensembles de données de RI conversationnelle à grande échelle à partir d’ensembles de données de RI ad hoc afin de faciliter les explorations sur la RI conversationnelle. Nous effectuons une évaluation approfondie montrant la qualité et la pertinence des interactions générées pour chaque requête initiale. Cet article montre la faisabilité et l’utilité de l’augmentation des ensembles de données de RI ad-hoc pour la RI conversationnelle.</abstract>
      <url hash="3aeecf1a">2023.jeptalnrecital-coria.3</url>
      <language>fra</language>
      <bibkey>erbacher-etal-2023-augmentation</bibkey>
    </paper>
    <paper id="4">
      <title>Apprentissage de sous-espaces de préfixes</title>
      <author><first>Louis</first><last>Falissard</last></author>
      <author><first>Vincent</first><last>Guigue</last></author>
      <author><first>Laure</first><last>Soulier</last></author>
      <pages>59–73</pages>
      <abstract>Cet article propose une nouvelle façon d’ajuster des modèles de langue en “Few-shot learning” se basant sur une méthode d’optimisation récemment introduite en vision informatique, l’apprentissage de sous-espaces de modèles. Cette méthode, permettant de trouver non pas un point minimum local de la fonction coût dans l’espace des paramètres du modèle, mais tout un simplexe associé à des valeurs basses, présente typiquement des capacités de généralisation supérieures aux solutions obtenues par ajustement traditionnel. L’adaptation de cette méthode aux gros modèles de langue n’est pas triviale mais son application aux méthodes d’ajustement dites “Parameter Efficient” est quant à elle relativement naturelle. On propose de plus une façon innovante d’utiliser le simplexe de solution étudié afin de revisiter la notion de guidage de l’ajustement d’un modèle par l’inférence d’une métrique de validation, problématique d’actualité en “few-shot learning”. On montre finalement que ces différentes contributions centrées autour de l’ajustement de sous-espaces de modèles est empiriquement associée à un gain considérable en performances de généralisation sur les tâches de compréhension du langage du benchmark GLUE, dans un contexte de “few-shot learning”.</abstract>
      <url hash="63b2ee0a">2023.jeptalnrecital-coria.4</url>
      <language>fra</language>
      <bibkey>falissard-etal-2023-apprentissage</bibkey>
    </paper>
    <paper id="5">
      <title>Recherche cross-modale pour répondre à des questions visuelles</title>
      <author><first>Paul</first><last>Lerner</last></author>
      <author><first>Ferret</first><last>Olivier</last></author>
      <author><first>Camille</first><last>Guinaudeau</last></author>
      <pages>74–92</pages>
      <abstract>Répondre à des questions visuelles à propos d’entités nommées (KVQAE) est une tâche difficile qui demande de rechercher des informations dans une base de connaissances multimodale. Nous étudions ici comment traiter cette tâche avec une recherche cross-modale et sa combinaison avec une recherche mono-modale, en se focalisant sur le modèle CLIP, un modèle multimodal entraîné sur des images appareillées à leur légende textuelle. Nos résultats démontrent la supériorité de la recherche cross-modale, mais aussi la complémentarité des deux, qui peuvent être combinées facilement. Nous étudions également différentes manières d’ajuster CLIP et trouvons que l’optimisation cross-modale est la meilleure solution, étant en adéquation avec son pré-entraînement. Notre méthode surpasse les approches précédentes, tout en étant plus simple et moins coûteuse. Ces gains de performance sont étudiés intrinsèquement selon la pertinence des résultats de la recherche et extrinsèquement selon l’exactitude de la réponse extraite par un module externe. Nous discutons des différences entre ces métriques et de ses implications pour l’évaluation de la KVQAE.</abstract>
      <url hash="b6101ed9">2023.jeptalnrecital-coria.5</url>
      <language>fra</language>
      <bibkey>lerner-etal-2023-recherche</bibkey>
    </paper>
    <paper id="6">
      <title>Adaptation de domaine pour la recherche dense par annotation automatique</title>
      <author><first>Minghan</first><last>Li</last></author>
      <author><first>Eric</first><last>Gaussier</last></author>
      <pages>93–110</pages>
      <abstract>Bien que la recherche d’information neuronale ait connu des améliorations, les modèles de recherche dense ont une capacité de généralisation à de nouveaux domaines limitée, contrairement aux modèles basés sur l’interaction. Les approches d’apprentissage adversarial et de génération de requêtes n’ont pas résolu ce problème. Cet article propose une approche d’auto-supervision utilisant des étiquettes de pseudo-pertinence automatiquement générées pour le domaine cible. Le modèle T53B est utilisé pour réordonner une liste de documents fournie par BM25 afin d’obtenir une annotation des exemples positifs. L’extraction des exemples négatifs est effectuée en explorant différentes stratégies. Les expériences montrent que cette approche aide le modèle dense sur le domaine cible et améliore l’approche de génération de requêtes GPL.</abstract>
      <url hash="a1018d15">2023.jeptalnrecital-coria.6</url>
      <language>fra</language>
      <bibkey>li-gaussier-2023-adaptation</bibkey>
    </paper>
    <paper id="7">
      <title>Extraction d’entités nommées à partir de descriptions d’espèces</title>
      <author><first>Maya</first><last>Sahraoui</last></author>
      <author><first>Vincent</first><last>Guigue</last></author>
      <author><first>Régine</first><last>Vignes-Lebbe</last></author>
      <author><first>Marc</first><last>Pignal</last></author>
      <pages>111–126</pages>
      <abstract>Les descriptions d’espèces contiennent des informations importantes sur les caractéristiques morphologiques des espèces, mais l’extraction de connaissances structurées à partir de ces descriptions est souvent chronophage. Nous proposons un modèle texte-graphe adapté aux descriptions d’espèces en utilisant la reconnaissance d’entités nommées (NER) faiblement supervisée. Après avoir extrait les entités nommées, nous reconstruisons les triplets en utilisant des règles de dépendance pour créer le graphe. Notre méthode permet de comparer différentes espèces sur la base de caractères morphologiques et de relier différentes sources de données. Les résultats de notre étude se concentrent sur notre modèle NER et démontrent qu’il est plus performant que les modèles de référence et qu’il constitue un outil précieux pour la communauté de l’écologie et de la biodiversité.</abstract>
      <url hash="8817be75">2023.jeptalnrecital-coria.7</url>
      <language>fra</language>
      <bibkey>sahraoui-etal-2023-extraction</bibkey>
    </paper>
    <paper id="8">
      <title>Le théâtre français du <fixed-case>XVII</fixed-case>e siècle : une expérience en catégorisation de textes</title>
      <author><first>Jacques</first><last>Savoy</last></author>
      <pages>127–138</pages>
      <abstract>La catégorisation de documents (attribution d’un texte à une ou plusieurs catégories prédéfinies) possède de multiples applications. Cette communication se focalise sur l’attribution d’auteur en analysant le style de vingt pièces de théâtre du XVIIe siècle. L’hypothèse que nous souhaitons vérifier admet que le véritable auteur est le nom apparaissant sur la couverture. Afin de vérifier la qualité de deux méthodes d’attribution, nous avons repris deux corpus additionnels basés sur des romans écrits en français et italien. Nous proposons une amélioration de la méthode Delta ainsi qu’une nouvelle grille d’analyse pour cette approche. Ensuite, nous avons appliqué ces approches sur notre collection de comédies. Les résultats démontrent que l’hypothèse de base doit être écartée. De plus, ces œuvres présentent des styles proches rendant toute attribution difficile.</abstract>
      <url hash="b18d8cf5">2023.jeptalnrecital-coria.8</url>
      <language>fra</language>
      <bibkey>savoy-2023-le</bibkey>
    </paper>
    <paper id="9">
      <title>Enrichissement des modèles de langue pré-entraînés par la distillation mutuelle des connaissances</title>
      <author><first>Raphaël</first><last>Sourty</last></author>
      <author><first>Jose G</first><last>Moreno</last></author>
      <author><first>François-Paul</first><last>Servant</last></author>
      <author><first>Lynda</first><last>Tamine</last></author>
      <pages>139–156</pages>
      <abstract>Les bases de connaissances sont des ressources essentielles dans un large éventail d’applications à forte intensité de connaissances. Cependant, leur incomplétude limite intrinsèquement leur utilisation et souligne l’importance de les compléter. À cette fin, la littérature a récemment adopté un point de vue de monde ouvert en associant la capacité des bases de connaissances à représenter des connaissances factuelles aux capacités des modèles de langage pré-entraînés (PLM) à capturer des connaissances linguistiques de haut niveau et contextuelles à partir de corpus de textes. Dans ce travail, nous proposons un cadre de distillation pour la complétion des bases de connaissances où les PLMs exploitent les étiquettes souples sous la forme de prédictions d’entités et de relations fournies par un modèle de plongements de bases de connaissances, tout en conservant leur pouvoir de prédiction d’entités sur de grandes collections des textes. Pour mieux s’adapter à la tâche de complétion des connaissances, nous étendons la modélisation traditionnelle du langage masqué des PLM à la prédiction d’entités et d’entités liées dans le contexte. Des expériences utilisant les tâches à forte intensité de connaissances dans le cadre du benchmark d’évaluation KILT montrent le potentiel de notre approche.</abstract>
      <url hash="2323fe21">2023.jeptalnrecital-coria.9</url>
      <language>fra</language>
      <bibkey>sourty-etal-2023-enrichissement</bibkey>
    </paper>
    <paper id="10">
      <title>Constitution de sous-fils de conversations d’emails</title>
      <author><first>Lionel</first><last>Tadonfouet Tadjou</last></author>
      <author><first>Eric</first><last>De La Clergerie</last></author>
      <author><first>Fabrice</first><last>Bourge</last></author>
      <author><first>Tiphaine</first><last>Marie</last></author>
      <pages>157–171</pages>
      <abstract>Les conversations d’emails en entreprise sont parfois difficiles à suivre par les collaborateurs car elles peuvent traiter de plusieurs sujets à la fois et impliquer de nombreux interlocuteurs. Pour faciliter la compréhension des messages clés, il est utile de créer des sous-fils de conversations. Dans notre étude, nous proposons un pipeline en deux étapes pour reconnaître les actes de dialogue dans les segments de texte d’une conversation et les relier pour améliorer l’accessibilité de l’information. Ce pipeline construit ainsi des paires de segments de texte transverses sur les emails d’une conversationfacilitant ainsi la compréhension des messages clés inhérents à celle-ci. A notre connaissance, c’est la première fois que cette problématique de constitution de fils de conversations est abordée sur les conversations d’emails. Nous avons annoté le corpus d’emails BC3 en actes de dialogues et mis enrelation les segments de texte de conversation d’emails de BC3.</abstract>
      <url hash="9552bf01">2023.jeptalnrecital-coria.10</url>
      <language>fra</language>
      <bibkey>tadonfouet-tadjou-etal-2023-constitution</bibkey>
    </paper>
    <paper id="11">
      <title>Intégration du raisonnement numérique dans les modèles de langue : État de l’art et direction de recherche</title>
      <author><first>Sarah</first><last>Abchiche</last></author>
      <author><first>Lynda</first><last>Said Lhadj</last></author>
      <author><first>Vincent</first><last>Guigue</last></author>
      <author><first>Laure</first><last>Soulier</last></author>
      <pages>173–184</pages>
      <abstract>Ces dernières années, les modèles de langue ont connu une évolution galopante grâce à l’augmentation de la puissance de calcul qui a rendu possible l’utilisation des réseaux de neurones. Parallèlement, l’intégration du raisonnement numérique dans les modèles de langue a suscité un intérêt grandissant. Pourtant, bien que l’entraînement des modèles de langue sur des données numériques soit devenu un paradigme courant, les modèles actuels ne parviennent pas à effectuer des calculs de manière satisfaisante. Pour y remédier, une solution est d’entraîner les modèles de langue à utiliser des outils externes tels qu’une calculatrice ou un “runtime” de code python pour effectuer le raisonnement numérique. L’objectif de ce papier est double, dans un premier temps nous passons en revue les travaux de l’état de l’art sur le raisonnement numérique dans les modèles de langue et dans un second temps nous discutons des différentes perspectives de recherche pour augmenter les compétences numériques des modèles.</abstract>
      <url hash="84894bd1">2023.jeptalnrecital-coria.11</url>
      <language>fra</language>
      <bibkey>abchiche-etal-2023-integration</bibkey>
    </paper>
    <paper id="12">
      <title>Reconnaissance d’Entités Nommées fondée sur des Modèles de Langue Enrichis avec des Définitions des Types d’Entités</title>
      <author><first>Jesús</first><last>Lovón Melgarejo</last></author>
      <author><first>Jose</first><last>Moreno</last></author>
      <author><first>Romaric</first><last>Besançon</last></author>
      <author><first>Olivier</first><last>Ferret</last></author>
      <author><first>Lynda</first><last>Tamine</last></author>
      <pages>185–194</pages>
      <abstract>Des études récentes ont identifié de nouveaux défis dans la tâche de reconnaissance d’entités nommées (NER), tels que la reconnaissance d’entités complexes qui ne sont pas des phrases nominales simples et/ou figurent dans des entrées textuelles courtes, avec une faible quantité d’informations contextuelles. Cet article propose une nouvelle approche qui relève ce défi, en se basant sur des modèles de langues pré-entraînés par enrichissement des définitions des types d’entités issus d’une base de connaissances. Les expériences menées dans le cadre de la tâche MultiCoNER I de SemEval ont montré que l’approche proposée permet d’atteindre des gains en performance par rapport aux modèles de référence de la tâche.</abstract>
      <url hash="b743ee36">2023.jeptalnrecital-coria.12</url>
      <language>fra</language>
      <bibkey>lovon-melgarejo-etal-2023-reconnaissance</bibkey>
    </paper>
    <paper id="13">
      <title>Entity Enhanced Attention Graph-Based Passages Retrieval</title>
      <author><first>Lucas</first><last>Albarede</last></author>
      <author><first>Lorraine</first><last>Goeuriot</last></author>
      <author><first>Philippe</first><last>Mulhem</last></author>
      <author><first>Claude</first><last>Le Pape-Gardeux</last></author>
      <author><first>Sylvain</first><last>Marie</last></author>
      <author><first>Trinidad</first><last>Chardin-Segui</last></author>
      <pages>196–200</pages>
      <abstract>Passage retrieval is crucial in specialized domains where documents are long and complex, such as patents, legal documents, scientific reports, etc. We explore in this paper the integration of Entities and passages in Heterogeneous Attention Graph Models dedicated to passage retrieval. We use the two passage retrieval architectures based on re-ranking proposed in [1]. We experiment our proposal on the TREC CAR Y3 Passage Retrieval Task. The results obtained show an improvement over state-of-the-art techniques and proves the effectiveness of the approach. Our experiments also show the importance of using adequate parameters for such approach.</abstract>
      <url hash="2a56f989">2023.jeptalnrecital-coria.13</url>
      <bibkey>albarede-etal-2023-entity</bibkey>
    </paper>
    <paper id="14">
      <title>Highlighting exact matching via marking strategies for ad hoc document ranking with pretrained contextualized language models</title>
      <author><first>Lila</first><last>Boualili</last></author>
      <author><first>Jose</first><last>Moreno</last></author>
      <author><first>Mohand</first><last>Boughanem</last></author>
      <pages>201–201</pages>
      <abstract>Les modèles de langue pré-entraînés (MLPs) à l’instar de BERT se sont révélés remarquablement efficaces pour le classement ad hoc. Contrairement aux modèles antérieurs à BERT qui nécessitent des composants neuronaux spécialisés pour capturer les différents aspects de la pertinence entre la requête et le document, les MLPs sont uniquement basés sur des blocs de “transformers” où l’attention est le seul mécanisme utilisé pour extraire des signaux à partir des interactions entre les termes de la requête et le document. Grâce à l’attention croisée du “transformer”, BERT s’est avéré être un modèle d’appariement sémantique efficace. Cependant, l’appariement exact reste un signal essentiel pour évaluer la pertinence d’un document par rapport à une requête de recherche d’informations, en dehors de l’appariement sémantique. Dans cet article, nous partons de l’hypothèse que BERT pourrait bénéficier d’indices explicites d’appariement exact pour mieux s’adapter à la tâche d’estimation de pertinence. Dans ce travail, nous explorons des stratégies d’intégration des signaux d’appariement exact en utilisant des “tokens” de marquage permettant de mettre en évidence les correspondances exactes entre les termes de la requête et ceux du document. Nous constatons que cette approche de marquage simple améliore de manière significative le modèle BERT vanille de référence. Nous démontrons empiriquement l’efficacité de notre approche par le biais d’expériences exhaustives sur trois collections standards en recherche d’information (RI). Les résultats montrent que les indices explicites de correspondance exacte transmis par le marquage sont bénéfiques pour des MLPs aussi bien BERT que pour ELECTRA. Nos résultats confirment que les indices traditionnels de RI, tels que la correspondance exacte de termes, sont toujours utiles pour les nouveaux modèles contextualisés pré-entraînés tels que BERT.</abstract>
      <url hash="f31af2c8">2023.jeptalnrecital-coria.14</url>
      <bibkey>boualili-etal-2023-highlighting</bibkey>
    </paper>
    <paper id="15">
      <title>Vers l’évaluation continue des systèmes de recherche d’information.</title>
      <author><first>Petra</first><last>Galuscakova</last></author>
      <author><first>Romain</first><last>Deveaud</last></author>
      <author><first>Gabriela</first><last>Gonzalez-Saez</last></author>
      <author><first>Philippe</first><last>Mulhem</last></author>
      <author><first>Lorraine</first><last>Goeuriot</last></author>
      <author><first>Florina</first><last>Piroi</last></author>
      <author><first>Martin</first><last>Popel</last></author>
      <pages>202–206</pages>
      <abstract>Cet article présente le corpus de données associé à la première campagne évaluation LongEval dans le cadre de CLEF 2023. L’objectif de cette évaluation est d’étudier comment les systèmes de recherche d’informations réagissent à l’évolution des données qu’ils manipulent (notamment les documents et les requêtes). Nous détaillons les objectifs de la tâche, le processus d’acquisition des données et les mesures d’évaluation utilisées.</abstract>
      <url hash="344e039f">2023.jeptalnrecital-coria.15</url>
      <language>fra</language>
      <bibkey>galuscakova-etal-2023-vers</bibkey>
    </paper>
    <paper id="16">
      <title><fixed-case>C</fixed-case>o<fixed-case>SPLADE</fixed-case> : Adaptation d’un Modèle Neuronal Basé sur des Représentations Parcimonieuses pour la Recherche d’Information Conversationnelle</title>
      <author><first>Nam</first><last>Le Hai</last></author>
      <author><first>Thomas</first><last>Gerald</last></author>
      <author><first>Thibault</first><last>Formal</last></author>
      <author><first>Jian-Yun</first><last>Nie</last></author>
      <author><first>Benjamin</first><last>Piwowarksi</last></author>
      <author><first>Laure</first><last>Soulier</last></author>
      <pages>207–212</pages>
      <abstract>La recherche conversationnelle est une tâche qui vise à retrouver des documents à partir de la questioncourante de l’utilisateur ainsi que l’historique complet de la conversation. La plupart des méthodesantérieures sont basées sur une approche multi-étapes reposant sur une reformulation de la question.Cette étape de reformulation est critique, car elle peut conduire à un classement sous-optimal des do-cuments. D’autres approches ont essayé d’ordonner directement les documents, mais s’appuient pourla plupart sur un jeu de données contenant des pseudo-labels. Dans ce travail, nous proposons une tech-nique d’apprentissage à la fois “légère” et innovante pour un modèle contextualisé d’ordonnancementbasé sur SPLADE. En s’appuyant sur les représentations parcimonieuses de SPLADE, nous montronsque notre modèle, lorsqu’il est combiné avec le modèle de ré-ordonnancement T5Mono, obtient desrésultats qui sont compétitifs avec ceux obtenus par les participants des campagnes d’évaluation TRECCAsT 2020 et 2021. Le code source est disponible sur https://github.com/anonymous.</abstract>
      <url hash="76554917">2023.jeptalnrecital-coria.16</url>
      <language>fra</language>
      <bibkey>le-hai-etal-2023-cosplade</bibkey>
    </paper>
    <paper id="17">
      <title>The Power of Selecting Key Blocks with Local Pre-ranking for Long Document Information Retrieval</title>
      <author><first>Minghan</first><last>Li</last></author>
      <author><first>Diana Nicoleta</first><last>Popa</last></author>
      <author><first>Johan</first><last>Chagnon</last></author>
      <author><first>Yagmur Gizem</first><last>Cinar</last></author>
      <author><first>Eric</first><last>Gaussier</last></author>
      <pages>213–213</pages>
      <abstract>Les réseaux neuronaux profonds et les modèles fondés sur les transformeurs comme BERT ont envahi le domaine de la recherche d’informations (RI) ces dernières années. Leur succès est lié au mécanisme d’auto-attention qui permet de capturer les dépendances entre les mots indépendamment de leur distance. Cependant, en raison de sa complexité quadratique dans le nombre de mots, ce mécanisme ne peut être directement utilisé sur de longues séquences, ce qui ne permet pas de déployer entièrement les modèles neuronaux sur des documents longs pouvant contenir des milliers de mots. Trois stratégies standard ont été adoptées pour contourner ce problème. La première consiste à tronquer les documents longs, la deuxième à segmenter les documents longs en passages plus courts et la dernière à remplacer le module d’auto-attention par des modules d’attention parcimonieux. Dans le premier cas, des informations importantes peuvent être perdues et le jugement de pertinence n’est fondé que sur une partie de l’information contenue dans le document. Dans le deuxième cas, une architecture hiérarchique peut être adoptée pour construire une représentation du document sur la base des représentations de chaque passage. Cela dit, malgré ses résultats prometteurs, cette stratégie reste coûteuse en temps, en mémoire et en énergie. Dans le troisième cas, les contraintes de parcimonie peuvent conduire à manquer des dépendances importantes et, in fine, à des résultats sous-optimaux. L’approche que nous proposons est légèrement différente de ces stratégies et vise à capturer, dans les documents longs, les blocs les plus importants permettant de décider du statut, pertinent ou non, de l’ensemble du document. Elle repose sur trois étapes principales : (a) la sélection de blocs clés (c’est-à-dire susceptibles d’être pertinents) avec un pré-classement local en utilisant soit des modèles de RI classiques, soit un module d’apprentissage, (b) l’apprentissage d’une représentation conjointe des requêtes et des blocs clés à l’aide d’un modèle BERT standard, et (c) le calcul d’un score de pertinence final qui peut être considéré comme une agrégation d’informations de pertinence locale. Dans cet article, nous menons tout d’abord une analyse qui révèle que les signaux de pertinence peuvent apparaître à différents endroits dans les documents et que de tels signaux sont mieux capturés par des relations sémantiques que par des correspondances exactes. Nous examinons ensuite plusieurs méthodes pour sélectionner les blocs pertinents et montrons comment intégrer ces méthodes dans les modèles récents de RI.</abstract>
      <url hash="43d3316e">2023.jeptalnrecital-coria.17</url>
      <bibkey>li-etal-2023-power</bibkey>
    </paper>
    <paper id="18">
      <title>i<fixed-case>QPP</fixed-case>: Une Référence pour la Prédiction de Performances des Requêtes d’Images</title>
      <author><first>Eduard</first><last>Poesina</last></author>
      <author><first>Radu Tudor</first><last>Ionescu</last></author>
      <author><first>Josiane</first><last>Mothe</last></author>
      <pages>214–220</pages>
      <abstract>La prédiction de la performance des requêtes (QPP) dans le contexte de la recherche d’images basée sur le contenu reste une tâche largement inexplorée, en particulier dans le scénario de la recherche par l’exemple, où la requête est une image. Pour stimuler les recherches dans ce domaine, nous proposons la première collection de référence. Nous proposons un ensemble de quatre jeux de données (PASCAL VOC 2012, Caltech-101, ROxford5k et RParis6k) avec les performances attendues pour chaque requête à l’aide de deux modèles de recherche d’images état de l’art. Nous proposons également de nouveaux prédicteurs pré et post-recherche. Les résultats empiriques montrent que la plupart des prédicteurs ne se généralisent pas aux différents scénarios d’évaluation. Nos expériences exhaustives indiquent que l’iQPP est une référence difficile, révélant une importante lacune dans la recherche qui doit être abordée dans les travaux futurs. Nous publions notre code et nos données.</abstract>
      <url hash="00b98e85">2023.jeptalnrecital-coria.18</url>
      <language>fra</language>
      <bibkey>poesina-etal-2023-iqpp</bibkey>
    </paper>
    <paper id="19">
      <title><fixed-case>XPMIR</fixed-case>: Une bibliothèque modulaire pour l’apprentissage d’ordonnancement et les expériences de <fixed-case>RI</fixed-case> neuronale</title>
      <author><first>Yuxuan</first><last>Zong</last></author>
      <author><first>Benjamin</first><last>Piwowarski</last></author>
      <pages>222–233</pages>
      <abstract>Ces dernières années, plusieurs librairies pour la recherche d’information (neuronale) ont été proposées. Cependant, bien qu’elles permettent de reproduire des résultats déjà publiés, il est encore très difficile de réutiliser certaines parties des chaînes de traitement d’apprentissage, comme par exemple le pré-entraînement, la stratégie d’échantillonnage ou la définition du coût dans les modèles nouvellement développés. Il est également difficile d’utiliser de nouvelles techniques d’apprentissage avec d’anciens modèles, ce qui complique l’évaluation de l’utilité des nouvelles idées pour les différents modèles de RI neuronaux. Cela ralentit l’adoption de nouvelles techniques et, par conséquent, le développement du domaine de la RI. Dans cet article, nous présentons XPMIR, une librairie Python définissant un ensemble réutilisable de composants expérimentaux. La bibliothèque contient déjà des modèles et des techniques d’indexation de pointe, et est intégrée au hub HuggingFace.</abstract>
      <url hash="1ee7b885">2023.jeptalnrecital-coria.19</url>
      <language>fra</language>
      <bibkey>zong-piwowarski-2023-xpmir</bibkey>
    </paper>
  </volume>
</collection>
